#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°çº¢ä¹¦åšä¸»æ–°å¸–å®æ—¶ç›‘æ§è„šæœ¬

åŠŸèƒ½ï¼š
  - æŒç»­ç›‘æ§æŒ‡å®šå°çº¢ä¹¦åšä¸»çš„ä¸»é¡µ
  - ä¸€æ—¦å‘ç°æ–°ç¬”è®°ç«‹å³ä¸‹è½½ï¼ˆå›¾ç‰‡/è§†é¢‘/æ–‡æ¡ˆï¼‰
  - é€šè¿‡ Bark æ¨é€æ–°å¸–é€šçŸ¥åˆ°æ‰‹æœº

ä½¿ç”¨æ–¹å¼ï¼š
  # è¯»å– .env ä¸­ XHS_MONITOR_USERS é…ç½®è¿è¡Œ
  python xhs_monitor.py

  # æŒ‡å®šåšä¸»è¿è¡Œ
  python xhs_monitor.py --users "https://www.xiaohongshu.com/user/profile/xxx"

  # è‡ªå®šä¹‰æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰
  python xhs_monitor.py --interval 300

  # ä»…æ£€æŸ¥ä¸€æ¬¡ä¸å¾ªç¯
  python xhs_monitor.py --once

  # æŒ‡å®šå¤šä¸ªåšä¸»
  python xhs_monitor.py --users "url1,url2"
"""

import argparse
import asyncio
import json
import logging
import os
import re
import sys
import time
import httpx
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Set
from dotenv import load_dotenv

# å°† XHS æ¨¡å—è·¯å¾„åŠ å…¥ Python æœç´¢è·¯å¾„ï¼ˆå¿…é¡»åœ¨ import XHSDownloader ä¹‹å‰ï¼‰
sys.path.insert(0, str(Path(__file__).parent / "XHS"))
from XHS.xhs_downloader import XHSDownloader

# ==========================================
# æ—¥å¿—é…ç½®
# ==========================================
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
    handlers=[
        logging.StreamHandler(sys.stdout),
    ],
)
logger = logging.getLogger("xhs_monitor")

# ==========================================
# å¸¸é‡
# ==========================================
USER_URL_PATTERN = re.compile(
    r"(?:https?://)?(?:www\.)?xiaohongshu\.com/user/profile/([a-zA-Z0-9_-]+)"
)


# ==========================================
# Bark æ¨é€å·¥å…·
# ==========================================
class BarkNotifier:
    """Bark iOS æ¨é€é€šçŸ¥"""

    def __init__(self, bark_key: str = ""):
        self.bark_key = bark_key or os.getenv("BARK_KEY", "").strip()
        self.base_url = "https://api.day.app"

    def is_enabled(self) -> bool:
        return bool(self.bark_key)

    async def push(
            self,
            title: str,
            body: str,
            url: str = "",
            group: str = "å°çº¢ä¹¦ç›‘æ§",
    ) -> bool:
        """å‘é€ Bark æ¨é€é€šçŸ¥ï¼ˆPOST æ–¹å¼ï¼‰"""
        if not self.is_enabled():
            logger.warning("[Bark] æœªé…ç½® BARK_KEYï¼Œè·³è¿‡æ¨é€")
            return False

        payload: Dict = {
            "title": title,
            "body": body,
            "group": group,
            "sound": "minuet",
        }
        if url:
            payload["url"] = url

        try:
            async with httpx.AsyncClient(timeout=10) as client:
                resp = await client.post(
                    f"{self.base_url}/{self.bark_key}",
                    json=payload,
                )
                if resp.status_code == 200:
                    logger.info(f"[Bark] æ¨é€æˆåŠŸ: {title}")
                    return True
                else:
                    logger.warning(f"[Bark] æ¨é€å¤±è´¥ HTTP {resp.status_code}: {resp.text}")
        except Exception as e:
            logger.error(f"[Bark] æ¨é€å¼‚å¸¸: {e}")
        return False


# ==========================================
# å•åšä¸»ç›‘æ§å™¨
# ==========================================
class XHSBloggerMonitor:
    """
    ç›‘æ§å•ä¸ªå°çº¢ä¹¦åšä¸»ï¼Œæ£€æµ‹æ–°ç¬”è®°å¹¶ä¸‹è½½ã€‚
    """

    def __init__(
            self,
            user_url: str,
            download_dir: str,
            downloader: XHSDownloader,
            notifier: BarkNotifier,
            seen_file_dir: Path,
            cookie: str = "",
    ):
        """
        Args:
            user_url: åšä¸»ä¸»é¡µå®Œæ•´ URL
            download_dir: ä¸‹è½½æ ¹ç›®å½•
            downloader: å·²åˆå§‹åŒ–çš„ XHSDownloader å®ä¾‹
            notifier: Bark æ¨é€å®ä¾‹
            seen_file_dir: å·²çŸ¥ç¬”è®° ID è®°å½•æ–‡ä»¶ç›®å½•
            cookie: å°çº¢ä¹¦ Cookie
        """
        self.user_url = user_url
        self.user_id = self._extract_user_id(user_url)
        self.download_dir = Path(download_dir)
        self.downloader = downloader
        self.notifier = notifier
        self.cookie = cookie or os.getenv("XHS_COOKIE", "")

        # å·²çŸ¥ç¬”è®° ID æŒä¹…åŒ–æ–‡ä»¶
        self.seen_file = seen_file_dir / f"{self.user_id}_seen.json"

        # åšä¸»æ˜µç§°ï¼ˆé¦–æ¬¡è·å–åç¼“å­˜ï¼‰
        self.author_name: str = ""

        # å†…å­˜ä¸­çš„å·²çŸ¥ç¬”è®°é›†åˆ
        self._seen_ids: Set[str] = self._load_seen_ids()

        logger.info(f"[åˆå§‹åŒ–] åšä¸» {self.user_id}ï¼Œå·²çŸ¥ç¬”è®°æ•°: {len(self._seen_ids)}")

    @staticmethod
    def _extract_user_id(url: str) -> str:
        """ä» URL ä¸­æå–ç”¨æˆ· ID"""
        match = USER_URL_PATTERN.search(url)
        return match.group(1) if match else url.strip()

    def _load_seen_ids(self) -> Set[str]:
        """ä»ç£ç›˜åŠ è½½å·²çŸ¥ç¬”è®° ID"""
        if not self.seen_file.exists():
            return set()
        try:
            with open(self.seen_file, "r", encoding="utf-8") as f:
                data = json.load(f)
                return set(data.get("seen_ids", []))
        except Exception as e:
            logger.warning(f"[è®°å½•] è¯»å–è®°å½•æ–‡ä»¶å¤±è´¥: {e}")
            return set()

    def _save_seen_ids(self):
        """å°†å·²çŸ¥ç¬”è®° ID æŒä¹…åŒ–åˆ°ç£ç›˜"""
        try:
            self.seen_file.parent.mkdir(parents=True, exist_ok=True)
            with open(self.seen_file, "w", encoding="utf-8") as f:
                json.dump(
                    {
                        "user_id": self.user_id,
                        "author_name": self.author_name,
                        "seen_ids": list(self._seen_ids),
                        "updated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    },
                    f,
                    ensure_ascii=False,
                    indent=2,
                )
        except Exception as e:
            logger.error(f"[è®°å½•] ä¿å­˜è®°å½•æ–‡ä»¶å¤±è´¥: {e}")

    async def _create_browser_context(self):
        """åˆ›å»ºå¹¶é…ç½® Playwright æµè§ˆå™¨ä¸Šä¸‹æ–‡"""
        from playwright.async_api import async_playwright

        pw = await async_playwright().start()
        browser = await pw.chromium.launch(
            headless=False,
            args=[
                "--disable-blink-features=AutomationControlled",
                "--no-sandbox",
                "--disable-dev-shm-usage",
                "--disable-infobars",
                "--window-size=1280,900",
            ],
        )
        context = await browser.new_context(
            user_agent=(
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/120.0.0.0 Safari/537.36"
            ),
            viewport={"width": 1280, "height": 900},
            locale="zh-CN",
        )
        await context.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
            window.chrome = {runtime: {}};
        """)

        # æ³¨å…¥ Cookie
        if self.cookie:
            cookies = []
            for item in self.cookie.split(";"):
                item = item.strip()
                if "=" in item:
                    name, value = item.split("=", 1)
                    cookies.append({
                        "name": name.strip(),
                        "value": value.strip(),
                        "domain": ".xiaohongshu.com",
                        "path": "/",
                    })
            if cookies:
                await context.add_cookies(cookies)
                logger.info(f"[æµè§ˆå™¨] å·²æ³¨å…¥ {len(cookies)} ä¸ª Cookie")

        return pw, browser, context

    async def _get_note_ids_from_dom(self, page) -> List[Dict]:
        """ä»é¡µé¢ DOM ä¸­æå–ç¬”è®° ID åˆ—è¡¨"""
        card_infos = await page.evaluate("""
            () => {
                const results = [];
                const seen = new Set();
                const selectors = [
                    'section.note-item a[href*="/explore/"]',
                    'section.note-item a[href*="/discovery/item/"]',
                    'div.note-item a[href*="/explore/"]',
                    'a[href*="/explore/"]',
                    'a[href*="/discovery/item/"]',
                ];
                for (const sel of selectors) {
                    const links = document.querySelectorAll(sel);
                    for (const a of links) {
                        const href = a.href || '';
                        const m = href.match(/\\/(explore|discovery\\/item)\\/([a-f0-9]+)/i);
                        if (!m) continue;
                        const noteId = m[2];
                        if (seen.has(noteId)) continue;
                        seen.add(noteId);
                        const titleEl = a.querySelector('.title, .note-title, span, footer span');
                        const title = (titleEl ? titleEl.textContent : '') || '';
                        results.push({
                            note_id: noteId,
                            title: title.trim().substring(0, 100),
                        });
                    }
                    if (results.length > 0) break;
                }
                return results;
            }
        """)
        return card_infos or []

    async def _extract_note_content(self, page) -> Dict:
        """ä»å½“å‰æ‰“å¼€çš„ç¬”è®°è¯¦æƒ…é¡µæå–å†…å®¹ï¼ˆå›¾ç‰‡/è§†é¢‘/æ–‡æ¡ˆï¼‰"""
        return await page.evaluate("""
            () => {
                const result = {
                    title: '',
                    description: '',
                    images: [],
                    video: '',
                    author: '',
                };

                // æå–æ ‡é¢˜
                const titleEl = document.querySelector('#detail-title')
                              || document.querySelector('.title')
                              || document.querySelector('[class*="title"]');
                if (titleEl) result.title = titleEl.textContent.trim();

                // æå–æ–‡æ¡ˆæè¿°
                const descEl = document.querySelector('#detail-desc')
                             || document.querySelector('.desc, .content, .note-text')
                             || document.querySelector('[class*="desc"]');
                if (descEl) result.description = descEl.textContent.trim();

                // æå–å›¾ç‰‡ URLï¼ˆåŒ…å«å¤šç§æ¥æºï¼‰
                const imgSelectors = [
                    '.swiper-slide img[src]',
                    '.carousel img[src]',
                    '.note-image img[src]',
                    '.media-container img[src]',
                    'img[class*="note"][src]',
                    '.slide-item img[src]',
                ];
                const imgSeen = new Set();
                for (const sel of imgSelectors) {
                    document.querySelectorAll(sel).forEach(img => {
                        let src = img.src || img.getAttribute('data-src') || '';
                        // è¿‡æ»¤æ‰å¤´åƒç­‰å°å›¾
                        if (src && !imgSeen.has(src) && !src.includes('avatar') 
                            && (src.includes('spectrum') || src.includes('ci.xiaohongshu') 
                                || src.includes('xhscdn') || src.includes('sns-img'))) {
                            imgSeen.add(src);
                            result.images.push(src);
                        }
                    });
                }

                // æå–è§†é¢‘ URL
                const videoEl = document.querySelector('video source[src]')
                              || document.querySelector('video[src]');
                if (videoEl) {
                    result.video = videoEl.src || videoEl.getAttribute('src') || '';
                }
                // ä¹Ÿå°è¯•ä» xgplayer æ’­æ”¾å™¨æå–
                if (!result.video) {
                    const xgVideo = document.querySelector('.xgplayer video');
                    if (xgVideo && xgVideo.src) result.video = xgVideo.src;
                }

                // æå–ä½œè€…å
                const authorEl = document.querySelector('.author-name, .username, [class*="author"] .name');
                if (authorEl) result.author = authorEl.textContent.trim();

                return result;
            }
        """)

    async def _download_media(self, urls: List[str], save_dir: Path, title: str) -> int:
        """ä¸‹è½½å›¾ç‰‡/è§†é¢‘æ–‡ä»¶"""
        downloaded = 0
        async with httpx.AsyncClient(
            headers={"User-Agent": "Mozilla/5.0", "Referer": "https://www.xiaohongshu.com/"},
            timeout=30, follow_redirects=True,
        ) as client:
            for i, url in enumerate(urls):
                if not url:
                    continue
                # ç¡®å®šæ–‡ä»¶æ‰©å±•å
                ext = "jpg"
                if "video" in url or ".mp4" in url:
                    ext = "mp4"
                elif ".webp" in url:
                    ext = "webp"
                elif ".png" in url:
                    ext = "png"

                safe_title = re.sub(r'[\\/:*?"<>|]', '_', title)[:50]
                filename = f"{safe_title}_{i}.{ext}"
                filepath = save_dir / filename

                if filepath.exists():
                    downloaded += 1
                    continue

                try:
                    resp = await client.get(url)
                    if resp.status_code == 200 and len(resp.content) > 1000:
                        filepath.write_bytes(resp.content)
                        downloaded += 1
                        logger.info(f"[ä¸‹è½½] âœ“ åª’ä½“æ–‡ä»¶: {filename} ({len(resp.content) // 1024}KB)")
                    else:
                        logger.warning(f"[ä¸‹è½½] âœ— åª’ä½“æ–‡ä»¶å¼‚å¸¸: HTTP {resp.status_code}, å¤§å° {len(resp.content)}")
                except Exception as e:
                    logger.warning(f"[ä¸‹è½½] âœ— åª’ä½“æ–‡ä»¶å¤±è´¥: {filename} â€” {e}")
        return downloaded

    async def check_and_download(self) -> int:
        """
        ä¸€ä½“åŒ–æ£€æŸ¥ + ä¸‹è½½ï¼šåœ¨åŒä¸€ä¸ª Playwright æµè§ˆå™¨ä¼šè¯ä¸­å®Œæˆã€‚
        1. è®¿é—®åšä¸»ä¸»é¡µè·å–ç¬”è®°åˆ—è¡¨
        2. å¯¹æ¯”å·²çŸ¥ç¬”è®°ï¼Œæ‰¾å‡ºæ–°å¸–
        3. ä¾æ¬¡ç‚¹å‡»æ–°å¸–å¡ç‰‡ï¼ˆè§¦å‘ Vue è·¯ç”±ï¼‰ï¼Œä»ç¬”è®°è¯¦æƒ…é¡µæå–å†…å®¹
        4. ä¸‹è½½å›¾ç‰‡/è§†é¢‘/æ–‡æ¡ˆ
        5. å…³é—­å¼¹çª—åå¤„ç†ä¸‹ä¸€ç¯‡

        Returns:
            æœ¬æ¬¡å‘ç°å¹¶å¤„ç†çš„æ–°ç¬”è®°æ•°é‡
        """
        logger.info(f"[æ£€æŸ¥] å¼€å§‹æ£€æŸ¥åšä¸»: {self.user_id}")

        try:
            from playwright.async_api import async_playwright
        except ImportError:
            logger.error("[Playwright] æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install playwright && playwright install chromium")
            return 0

        user_profile_url = f"https://www.xiaohongshu.com/user/profile/{self.user_id}"
        pw, browser, context = await self._create_browser_context()
        download_success = 0
        new_note_count = 0

        try:
            page = await context.new_page()
            logger.info(f"[æµè§ˆå™¨] å¯åŠ¨ Chromium æ£€æŸ¥åšä¸» {self.user_id}...")

            await page.goto(user_profile_url, wait_until="domcontentloaded", timeout=40000)
            await page.wait_for_timeout(5000)

            # æ»šåŠ¨åŠ è½½
            for _ in range(3):
                await page.evaluate("window.scrollBy(0, 800)")
                await asyncio.sleep(1.5)

            # ä» DOM è·å–ç¬”è®°åˆ—è¡¨
            card_infos = await self._get_note_ids_from_dom(page)
            logger.info(f"[DOM] å…±å‘ç° {len(card_infos)} æ¡ç¬”è®°")

            if not card_infos:
                logger.warning("[æ£€æŸ¥] æœªè·å–åˆ°ä»»ä½•ç¬”è®°ï¼Œå¯èƒ½æ˜¯ç™»å½•å¤±æ•ˆæˆ–åšä¸»æ— å†…å®¹")
                # æˆªå›¾è°ƒè¯•
                debug_path = str(self.download_dir / f"debug_{self.user_id}.png")
                try:
                    await page.screenshot(path=debug_path, full_page=True)
                    logger.warning(f"[è°ƒè¯•] å·²ä¿å­˜é¡µé¢æˆªå›¾: {debug_path}")
                except Exception:
                    pass
                return 0

            # æå–åšä¸»æ˜µç§°
            try:
                name = await page.evaluate("""
                    () => {
                        const el = document.querySelector('.user-name, .username, [class*="nickname"]');
                        return el ? el.textContent.trim() : '';
                    }
                """)
                if name and not self.author_name:
                    self.author_name = name
            except Exception:
                pass

            # è¿‡æ»¤å‡ºæ–°ç¬”è®°
            all_note_ids = [c["note_id"] for c in card_infos]
            new_notes = [c for c in card_infos if c["note_id"] not in self._seen_ids]

            if not new_notes:
                logger.info(f"[æ£€æŸ¥] æ— æ–°ç¬”è®°ï¼ˆå·²çŸ¥ {len(self._seen_ids)} ç¯‡ï¼‰")
                return 0

            logger.info(f"[å‘ç°] åšä¸» {self.author_name or self.user_id} æœ‰ {len(new_notes)} ç¯‡æ–°ç¬”è®°ï¼")

            # é¦–æ¬¡è¿è¡Œï¼šåªè®°å½•åŸºçº¿
            if len(self._seen_ids) == 0:
                logger.info("[é¦–æ¬¡] é¦–æ¬¡è¿è¡Œï¼Œè®°å½•å½“å‰æ‰€æœ‰ç¬”è®° ID ä½œä¸ºåŸºçº¿ï¼Œä¸æ‰§è¡Œä¸‹è½½")
                for nid in all_note_ids:
                    self._seen_ids.add(nid)
                self._save_seen_ids()
                logger.info(f"[é¦–æ¬¡] å·²è®°å½• {len(self._seen_ids)} ç¯‡ç¬”è®°ä¸ºåŸºçº¿")
                return 0

            new_note_count = len(new_notes)

            # ===== é€ä¸ªç‚¹å‡»æ–°ç¬”è®°å¡ç‰‡ï¼Œä»è¯¦æƒ…é¡µæå–å†…å®¹å¹¶ä¸‹è½½ =====
            for idx, note_info in enumerate(new_notes, 1):
                note_id = note_info["note_id"]
                title = note_info.get("title", "")
                label = self.author_name or self.user_id

                logger.info(f"[å¤„ç† {idx}/{len(new_notes)}] ç¬”è®° {note_id}: {title[:30]}")

                try:
                    # ç¡®ä¿åœ¨ä¸»é¡µ
                    if self.user_id not in page.url:
                        await page.goto(user_profile_url, wait_until="domcontentloaded", timeout=20000)
                        await page.wait_for_timeout(3000)

                    # æŸ¥æ‰¾ç¬”è®°å¡ç‰‡çš„å®¹å™¨å…ƒç´ ï¼ˆsection.note-item æˆ–å¤–å±‚ divï¼‰
                    # æ³¨æ„ï¼šç‚¹å‡»çš„æ˜¯å®¹å™¨è€Œä¸æ˜¯ <a> æ ‡ç­¾ï¼Œè¿™æ ·æ‰èƒ½è§¦å‘ Vue è·¯ç”±äº‹ä»¶
                    card_el = await page.query_selector(
                        f'section.note-item:has(a[href*="{note_id}"])'
                    ) or await page.query_selector(
                        f'div.note-item:has(a[href*="{note_id}"])'
                    ) or await page.query_selector(
                        f'a[href*="{note_id}"]'
                    )

                    if not card_el:
                        logger.warning(f"[å¤„ç†] æœªæ‰¾åˆ°ç¬”è®° {note_id} çš„å¡ç‰‡å…ƒç´ ")
                        self._seen_ids.add(note_id)
                        continue

                    # æ»šåŠ¨åˆ°å¯è§ä½ç½®å¹¶ç‚¹å‡»
                    await card_el.scroll_into_view_if_needed(timeout=5000)
                    await asyncio.sleep(0.5)
                    await card_el.click(timeout=10000)

                    # ç­‰å¾…ç¬”è®°è¯¦æƒ…åŠ è½½ï¼ˆURL åº”è¯¥å˜ä¸º /explore/{note_id}?xsec_token=...ï¼‰
                    await asyncio.sleep(3)
                    current_url = page.url
                    logger.info(f"[å¤„ç†] å¯¼èˆªå URL: {current_url}")

                    # æå–ç¬”è®°å†…å®¹
                    content = await self._extract_note_content(page)

                    note_title = content.get("title", "") or title or note_id
                    note_desc = content.get("description", "")
                    note_images = content.get("images", [])
                    note_video = content.get("video", "")
                    note_author = content.get("author", "") or label

                    logger.info(
                        f"[æå–] æ ‡é¢˜: {note_title[:30]} | "
                        f"å›¾ç‰‡: {len(note_images)} | "
                        f"è§†é¢‘: {'âœ“' if note_video else 'âœ—'} | "
                        f"æ–‡æ¡ˆ: {len(note_desc)} å­—"
                    )

                    # å‡†å¤‡ä¸‹è½½ç›®å½•
                    safe_author = re.sub(r'[\\/:*?"<>|]', '_', note_author)[:30]
                    safe_title = re.sub(r'[\\/:*?"<>|]', '_', note_title)[:50]
                    save_dir = self.download_dir / safe_author / f"{note_id}_{safe_title}"
                    save_dir.mkdir(parents=True, exist_ok=True)

                    # ä¿å­˜æ–‡æ¡ˆ
                    text_file = save_dir / f"{safe_title}.txt"
                    text_content = (
                        f"æ ‡é¢˜: {note_title}\n"
                        f"ä½œè€…: {note_author}\n"
                        f"ID: {note_id}\n"
                        f"URL: {current_url}\n"
                        f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
                        f"\n{note_desc}"
                    )
                    text_file.write_text(text_content, encoding="utf-8")

                    # ä¸‹è½½åª’ä½“æ–‡ä»¶
                    media_urls = []
                    if note_video:
                        media_urls.append(note_video)
                    media_urls.extend(note_images)

                    if media_urls:
                        dl_count = await self._download_media(media_urls, save_dir, safe_title)
                        logger.info(f"[ä¸‹è½½] åª’ä½“ä¸‹è½½å®Œæˆ: {dl_count}/{len(media_urls)}")
                    else:
                        logger.warning("[ä¸‹è½½] æœªæå–åˆ°ä»»ä½•åª’ä½“æ–‡ä»¶ URL")

                    download_success += 1
                    logger.info(f"[ä¸‹è½½] âœ“ æˆåŠŸ: {note_title[:30]}")

                    # æ¨é€é€šçŸ¥
                    await self.notifier.push(
                        title=f"ğŸ“• {label} å‘å¸ƒæ–°å¸–",
                        body=f"ã€Š{note_title}ã€‹\n{current_url}",
                        url=current_url,
                        group="å°çº¢ä¹¦ç›‘æ§",
                    )

                except Exception as e:
                    logger.error(f"[å¤„ç†] ç¬”è®° {note_id} å¤„ç†å¼‚å¸¸: {e}")
                    import traceback
                    traceback.print_exc()

                # æ— è®ºæˆåŠŸä¸å¦éƒ½è®°å½•
                self._seen_ids.add(note_id)

                # å›åˆ°ä¸»é¡µå‡†å¤‡ä¸‹ä¸€ä¸ª
                try:
                    await page.goto(user_profile_url, wait_until="domcontentloaded", timeout=20000)
                    await page.wait_for_timeout(2000)
                except Exception:
                    pass

                if idx < len(new_notes):
                    await asyncio.sleep(2)

        except Exception as e:
            logger.error(f"[æµè§ˆå™¨] æ•´ä½“å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
        finally:
            try:
                await browser.close()
            except Exception:
                pass
            try:
                await pw.stop()
            except Exception:
                pass

        self._save_seen_ids()
        logger.info(
            f"[å®Œæˆ] æœ¬è½®æ–°å¸–å¤„ç†å®Œæ¯•: å…± {new_note_count} ç¯‡ï¼ŒæˆåŠŸä¸‹è½½ {download_success} ç¯‡"
        )
        return new_note_count


# ==========================================
# å¤šåšä¸»è°ƒåº¦å™¨
# ==========================================
class XHSMonitorScheduler:
    """
    å¤šåšä¸»ç›‘æ§è°ƒåº¦å™¨ï¼Œæ”¯æŒå®šæ—¶è½®è¯¢ã€‚
    """

    def __init__(
            self,
            user_urls: List[str],
            interval: int = 600,
            download_dir: str = "downloads/xhs_monitor",
            cookie: str = "",
            bark_key: str = "",
            run_once: bool = False,
    ):
        """
        Args:
            user_urls: åšä¸»ä¸»é¡µ URL åˆ—è¡¨
            interval: è½®è¯¢é—´éš”ï¼ˆç§’ï¼‰
            download_dir: ä¸‹è½½æ ¹ç›®å½•
            cookie: å°çº¢ä¹¦ Cookie
            bark_key: Bark Key
            run_once: True åˆ™åªè¿è¡Œä¸€è½®åé€€å‡º
        """
        load_dotenv()

        self.user_urls = user_urls
        self.interval = interval
        self.run_once = run_once
        self.download_dir = Path(download_dir)

        # Cookie ä¼˜å…ˆçº§ï¼šå‚æ•° > .env
        self.cookie = cookie or os.getenv("XHS_COOKIE", "")

        # åˆ›å»ºç»Ÿä¸€çš„ä¸‹è½½å™¨ï¼ˆå¤ç”¨åŒä¸€ XHS sessionï¼‰
        self.downloader = XHSDownloader(
            cookie=self.cookie,
            download_dir=str(self.download_dir),
            skip_existing=True,
            download_image=True,
            download_video=True,
        )

        # åˆ›å»º Bark æ¨é€å™¨
        self.notifier = BarkNotifier(bark_key=bark_key)

        # å·²çŸ¥ç¬”è®°è®°å½•ç›®å½•
        seen_file_dir = self.download_dir / ".seen"
        seen_file_dir.mkdir(parents=True, exist_ok=True)

        # åˆå§‹åŒ–æ¯ä¸ªåšä¸»çš„ç›‘æ§å™¨
        self.monitors: List[XHSBloggerMonitor] = []
        for url in self.user_urls:
            if not USER_URL_PATTERN.search(url) and len(url) < 50:
                # å…¼å®¹ç›´æ¥ä¼ ç”¨æˆ· ID çš„æƒ…å†µ
                url = f"https://www.xiaohongshu.com/user/profile/{url}"
            monitor = XHSBloggerMonitor(
                user_url=url,
                download_dir=str(self.download_dir),
                downloader=self.downloader,
                notifier=self.notifier,
                seen_file_dir=seen_file_dir,
                cookie=self.cookie,
            )
            self.monitors.append(monitor)

    async def run_round(self):
        """æ‰§è¡Œä¸€è½®æ‰€æœ‰åšä¸»çš„æ£€æŸ¥"""
        if not self.monitors:
            logger.warning("[è°ƒåº¦] æ²¡æœ‰é…ç½®ä»»ä½•åšä¸»ï¼Œè¯·æ£€æŸ¥ --users å‚æ•°æˆ– XHS_MONITOR_USERS ç¯å¢ƒå˜é‡")
            return

        for monitor in self.monitors:
            try:
                await monitor.check_and_download()
            except Exception as e:
                logger.error(f"[è°ƒåº¦] åšä¸» {monitor.user_id} æ£€æŸ¥å¼‚å¸¸: {e}")
                import traceback
                traceback.print_exc()
            # å¤šåšä¸»ä¹‹é—´é€‚å½“é—´éš”
            if len(self.monitors) > 1:
                await asyncio.sleep(5)

    async def run(self):
        """å¯åŠ¨ç›‘æ§ä¸»å¾ªç¯"""
        self._print_banner()

        if self.run_once:
            logger.info("[è°ƒåº¦] å•æ¬¡æ¨¡å¼ï¼Œæ‰§è¡Œä¸€è½®åé€€å‡º")
            await self.run_round()
            logger.info("[è°ƒåº¦] å•æ¬¡æ£€æŸ¥å®Œæ¯•ï¼Œç¨‹åºé€€å‡º")
            return

        # æŒç»­å¾ªç¯
        round_num = 0
        while True:
            round_num += 1
            logger.info(f"\n{'=' * 60}")
            logger.info(f"  ç¬¬ {round_num} è½®æ£€æŸ¥  |  {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            logger.info(f"{'=' * 60}")

            await self.run_round()

            logger.info(f"[è°ƒåº¦] æœ¬è½®å®Œæˆï¼Œ{self.interval} ç§’åè¿›è¡Œä¸‹ä¸€è½®æ£€æŸ¥...")
            logger.info(
                f"[è°ƒåº¦] ä¸‹æ¬¡æ£€æŸ¥æ—¶é—´: {datetime.fromtimestamp(time.time() + self.interval).strftime('%H:%M:%S')}"
            )

            try:
                await asyncio.sleep(self.interval)
            except asyncio.CancelledError:
                logger.info("[è°ƒåº¦] ç›‘æ§å·²åœæ­¢")
                break

    def _print_banner(self):
        """æ‰“å°å¯åŠ¨ä¿¡æ¯"""
        print()
        print("=" * 60)
        print("  ğŸ” å°çº¢ä¹¦åšä¸»æ–°å¸–å®æ—¶ç›‘æ§")
        print("=" * 60)
        print(f"  ç›‘æ§åšä¸»æ•°: {len(self.monitors)}")
        for m in self.monitors:
            print(f"    - {m.user_id}")
        print(f"  æ£€æŸ¥é—´éš”: {self.interval} ç§’")
        print(f"  ä¸‹è½½ç›®å½•: {self.download_dir}")
        print(f"  Cookie:   {'âœ“ å·²é…ç½®' if self.cookie else 'âœ— æœªé…ç½®ï¼ˆå¯èƒ½å½±å“æ•ˆæœï¼‰'}")
        print(f"  Bark:     {'âœ“ å·²é…ç½®' if self.notifier.is_enabled() else 'âœ— æœªé…ç½®ï¼ˆä¸ä¼šæ¨é€é€šçŸ¥ï¼‰'}")
        print(f"  æ¨¡å¼:     {'å•æ¬¡' if self.run_once else 'æŒç»­å¾ªç¯'}")
        print("=" * 60)
        print()


# ==========================================
# å‘½ä»¤è¡Œå…¥å£
# ==========================================

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="å°çº¢ä¹¦åšä¸»æ–°å¸–å®æ—¶ç›‘æ§ â€” å‘ç°æ–°å¸–ç«‹å³ä¸‹è½½",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # ä½¿ç”¨ .env é…ç½®çš„åšä¸»åˆ—è¡¨ï¼ˆXHS_MONITOR_USERSï¼‰
  python xhs_monitor.py

  # æ‰‹åŠ¨æŒ‡å®šå•ä¸ªåšä¸»
  python xhs_monitor.py --users "https://www.xiaohongshu.com/user/profile/xxx"

  # ç›‘æ§å¤šä¸ªåšä¸»ï¼ˆé€—å·åˆ†éš”ï¼‰
  python xhs_monitor.py --users "url1,url2,url3"

  # æ¯ 5 åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
  python xhs_monitor.py --interval 300

  # ä»…æ£€æŸ¥ä¸€æ¬¡ï¼Œä¸å¾ªç¯
  python xhs_monitor.py --once

  # æŒ‡å®šä¸‹è½½ç›®å½•
  python xhs_monitor.py --download-dir "D:/å°çº¢ä¹¦ä¸‹è½½"
        """,
    )

    parser.add_argument(
        "--users",
        type=str,
        default="",
        help="åšä¸»ä¸»é¡µ URL åˆ—è¡¨ï¼ˆé€—å·åˆ†éš”ï¼‰ï¼Œå¦‚ä¸ä¼ åˆ™è¯»å– .env çš„ XHS_MONITOR_USERS",
    )
    parser.add_argument(
        "--interval",
        type=int,
        default=0,
        help="æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰ï¼Œå¦‚ä¸ä¼ åˆ™è¯»å– .env çš„ XHS_MONITOR_INTERVALï¼ˆé»˜è®¤ 600 ç§’ï¼‰",
    )
    parser.add_argument(
        "--download-dir",
        type=str,
        default="",
        help="ä¸‹è½½ç›®å½•ï¼Œå¦‚ä¸ä¼ åˆ™è¯»å– .env çš„ XHS_MONITOR_DIRï¼ˆé»˜è®¤ downloads/xhs_monitorï¼‰",
    )
    parser.add_argument(
        "--once",
        action="store_true",
        help="åªæ£€æŸ¥ä¸€æ¬¡åé€€å‡ºï¼ˆä¸å¾ªç¯ï¼‰",
    )

    return parser.parse_args()


async def main():
    """ä¸»å‡½æ•°"""
    load_dotenv()
    args = parse_args()

    # è§£æåšä¸»åˆ—è¡¨ï¼šå‘½ä»¤è¡Œ > .env
    raw_users = args.users or os.getenv("XHS_MONITOR_USERS", "")
    user_urls = [u.strip() for u in raw_users.split(",") if u.strip()]

    # é™çº§ï¼šå¦‚æœæ²¡æœ‰é…ç½®ç›‘æ§åˆ—è¡¨ï¼Œå°è¯•è¯»å–å†å²é…ç½® XHS_TARGET_URL ä½œä¸ºå•åšä¸»
    if not user_urls:
        fallback = os.getenv("XHS_TARGET_URL", "").strip()
        if fallback:
            logger.info(f"[é…ç½®] æœªé…ç½® XHS_MONITOR_USERSï¼Œä½¿ç”¨ XHS_TARGET_URL ä½œä¸ºå¤‡é€‰: {fallback}")
            user_urls = [fallback]
        else:
            logger.error("[é…ç½®] è¯·é€šè¿‡ --users å‚æ•°æˆ– .env çš„ XHS_MONITOR_USERS æŒ‡å®šè¦ç›‘æ§çš„åšä¸»")
            sys.exit(1)

    # è§£æé—´éš”
    interval = args.interval or int(os.getenv("XHS_MONITOR_INTERVAL", "600"))

    # è§£æä¸‹è½½ç›®å½•
    download_dir = args.download_dir or os.getenv("XHS_MONITOR_DIR", "downloads/xhs_monitor")

    scheduler = XHSMonitorScheduler(
        user_urls=user_urls,
        interval=interval,
        download_dir=download_dir,
        run_once=args.once,
    )

    try:
        await scheduler.run()
    except KeyboardInterrupt:
        print("\n\n[ä¸­æ–­] ç¨‹åºå·²è¢«ç”¨æˆ·åœæ­¢ (Ctrl+C)")


if __name__ == "__main__":
    asyncio.run(main())
